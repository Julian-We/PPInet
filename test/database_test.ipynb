{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SQLite with sqlalchemy\n",
    "Unfortunately this is an overkill for the current project, but it is a good practice to use it. (in general)"
   ],
   "id": "75f70aaa50d28872"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-19T09:22:22.617219Z",
     "start_time": "2024-04-19T09:22:22.612026Z"
    }
   },
   "source": [
    "# import numpy as np\n",
    "# from sqlalchemy import Column, String, Integer, REAL, create_engine\n",
    "# from sqlalchemy.orm import declarative_base\n",
    "# from sqlalchemy.orm import sessionmaker\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For the SQL Database we need the following:\n",
    "\n",
    "An object that has all the properties associated to a pairwise prediction\n",
    "\n",
    "    A) A unique identifier. Will be a hex string chosen from 10^10 numbers there will be a unocupied numbers file.\n",
    "    B) Path to folder\n",
    "    C) Both – gene names and gene identifiers, sequences, canonicality, Chain length\n",
    "    D) Predicted Values – pTM, ipTM, pDockQ, mpDockQ"
   ],
   "id": "75251617ca19bec0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T12:31:31.773717Z",
     "start_time": "2024-04-18T12:31:31.770589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class PairwisePrediction(Base):\n",
    "    __tablename__ = 'AF2 full-lib'\n",
    "\n",
    "    pid = Column(\"pid\", String, primary_key=True)\n",
    "    data_path = Column(\"data_path\", String)\n",
    "    geneA = Column(\"gene A\", String)\n",
    "    geneB = Column(\"gene B\", String)\n",
    "    nameA = Column(\"name A\", String)\n",
    "    nameB = Column(\"name B\", String)\n",
    "    seqA = Column(\"seq A\", String)\n",
    "    seqB = Column(\"seq B\", String)\n",
    "    canonicA = Column(\"canonic A\", Integer)\n",
    "    canonicB = Column(\"canonic B\", Integer)\n",
    "    lenA = Column(\"len A\", Integer)\n",
    "    lenB = Column(\"len B\", Integer)\n",
    "    pTM = Column(\"pTM\", REAL)\n",
    "    ipTM = Column(\"ipTM\", REAL)\n",
    "    pDockQ = Column(\"pDockQ\", REAL)\n",
    "    meanpDockQ = Column(\"meanpDockQ\", REAL)\n",
    "\n",
    "    def __init__(self, pid, data_path, geneA, geneB, nameA, nameB, seqA, seqB, canonicA, canonicB, lenA, lenB, pTM,\n",
    "                 ipTM, pDockQ, meanpDockQ, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.pid = pid\n",
    "        self.meanpDockQ = meanpDockQ\n",
    "        self.pDockQ = pDockQ\n",
    "        self.ipTM = ipTM\n",
    "        self.pTM = pTM\n",
    "        self.lenB = lenB\n",
    "        self.lenA = lenA\n",
    "        self.canonicB = canonicB\n",
    "        self.canonicA = canonicA\n",
    "        self.seqB = seqB\n",
    "        self.seqA = seqA\n",
    "        self.nameB = nameB\n",
    "        self.nameA = nameA\n",
    "        self.geneB = geneB\n",
    "        self.data_path = data_path\n",
    "        self.geneA = geneA\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__tablename__}({self.pid}, {self.data_path}, {self.geneA}, {self.geneB}, {self.nameA}, {self.nameB}, {self.seqA}, {self.seqB}, {self.canonicA}, {self.canonicB}, {self.lenA}, {self.lenB}, {self.pTM}, {self.ipTM}, {self.pDockQ}, {self.meanpDockQ})\"\n",
    "\n",
    "\n",
    "# %%\n",
    "engine = create_engine('sqlite:///PPIDB_full.db', echo=True)\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "interaction = PairwisePrediction(\n",
    "    pid=str(hex(np.random.randint(10 ** 10))),\n",
    "    data_path='path/to/data',\n",
    "    geneA='geneA',\n",
    "    geneB='geneB',\n",
    "    nameA='nameA',\n",
    "    nameB='nameB',\n",
    "    seqA='seqA',\n",
    "    seqB='seqB',\n",
    "    canonicA=1,\n",
    "    canonicB=1,\n",
    "    lenA=100,\n",
    "    lenB=100,\n",
    "    pTM=0.5,\n",
    "    ipTM=0.5,\n",
    "    pDockQ=0.5,\n",
    "    meanpDockQ=0.5)\n",
    "\n",
    "print(interaction)\n",
    "\n",
    "# %%\n",
    "# session.add(interaction)\n",
    "# session.commit()"
   ],
   "id": "f3109b4d09f7c08a",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pickle file ",
   "id": "6cf23529107beaca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T11:04:16.910860Z",
     "start_time": "2024-04-19T11:04:16.907978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import filecmp\n"
   ],
   "id": "8f587a359d73413",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1 – compare directories and identify the files that are not in the database",
   "id": "f9eeb71cce385516"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T11:04:17.933260Z",
     "start_time": "2024-04-19T11:04:17.929873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def identity_diff(base_path, previous_data, current_data):\n",
    "#     changed_items = {'added': [], 'removed': [], 'modified_files': {}}\n",
    "#     \n",
    "#     # Compare current data with previous data\n",
    "#     current_paths = set(current_data.keys())\n",
    "#     previous_paths = set(previous_data.keys())\n",
    "# \n",
    "#     # Identify added and removed paths\n",
    "#     added_paths = current_paths - previous_paths\n",
    "#     removed_paths = previous_paths - current_paths\n",
    "# \n",
    "#     for path_local in added_paths:\n",
    "#         changed_items['added'].append(path_local)\n",
    "#     for path_local in removed_paths:\n",
    "#         changed_items['removed'].append(path_local)\n",
    "# \n",
    "#     # Check for modifications in existing paths\n",
    "#     for path_local in current_paths.intersection(previous_paths):\n",
    "#         current_info = current_data[path_local]\n",
    "#         previous_info = previous_data[path_local]\n",
    "#         if current_info != previous_info:\n",
    "#             changed_items[path_local] = current_info\n",
    "#             modified_files = []\n",
    "#             for file in current_info['files']:\n",
    "#                 if file not in previous_info['files'] or \\\n",
    "#                     os.path.getmtime(os.path.join(base_path, path_local, file)) != previous_info['last_modified']:\n",
    "#                     modified_files.append(file)\n",
    "#             if modified_files:\n",
    "#                 changed_items['modified_files'][path_local] = modified_files\n",
    "#     \n",
    "#     return changed_items\n"
   ],
   "id": "98f95701f5c0fd0a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T11:04:19.023173Z",
     "start_time": "2024-04-19T11:04:18.532997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def analyze_db(dir_path):\n",
    "#     # Check if previous data exists\n",
    "#     previous_data = {}\n",
    "#     if os.path.exists(os.path.join(dir_path ,'dir_data.pickle')):\n",
    "#         with open(os.path.join(dir_path ,'dir_data.pickle'), 'rb') as f:\n",
    "#             previous_data = pickle.load(f)\n",
    "#     \n",
    "#     # Get current directory structure\n",
    "#     current_data = {}\n",
    "#     for root, dirs, files in os.walk(dir_path):\n",
    "#         relative_path = os.path.relpath(root, dir_path)\n",
    "#         current_data[relative_path] = {\n",
    "#             'dirs': dirs,\n",
    "#             'files': files,\n",
    "#             'last_modified': os.path.getmtime(root)\n",
    "#         }\n",
    "#     \n",
    "#     # Identify differences\n",
    "#     changes = identity_diff(dir_path, previous_data, current_data)\n",
    "#     \n",
    "#     # Clean MSA folder from changes for the remvoed folder/\n",
    "#     changes['removed'] = [x for x in changes['removed'] if 'msas' not in x]\n",
    "#     \n",
    "#     # Clean MSA folder from changes for the added folder\n",
    "#     changes['added'] = [x for x in changes['added'] if 'msas' not in x]\n",
    "#     # Save current data for future comparison\n",
    "#     with open(os.path.join(dir_path ,'dir_data.pickle'), 'wb') as f:\n",
    "#         pickle.dump(current_data, f)\n",
    "#     return changes\n",
    "# \n",
    "# \n",
    "# chgs = analyze_db('/Volumes/dave/JW/PPIDB/full_lib')\n",
    "\n"
   ],
   "id": "b28bee69ae5ddfd1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T10:40:59.282015Z",
     "start_time": "2024-04-19T10:40:59.275474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the_path = '/Volumes/dave/JW/PPIDB/full_lib/control_db'\n",
    "# list_exp_path = [os.path.join(the_path, exp_pth) for exp_pth in os.listdir(the_path) if\n",
    "#                      os.path.isdir(os.path.join(the_path, exp_pth)) and not exp_pth.startswith('.')]"
   ],
   "id": "fee3c0fc00c28a64",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Small scale test\n",
    "\n",
    "Removed paths have to be treated differently, as they are not present in the database."
   ],
   "id": "54172364f913b975"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T11:53:24.798200Z",
     "start_time": "2024-04-19T11:53:24.353535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from aflib.decipher import load_library_aslist, uniquify\n",
    "import sqlite3\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n"
   ],
   "id": "4298c528a57bd3fa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T11:53:25.611254Z",
     "start_time": "2024-04-19T11:53:25.605558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def identity_diff(base_path, previous_data, current_data):\n",
    "    changed_items = {'added': [], 'removed': [], 'modified_files': {}}\n",
    "    \n",
    "    # Compare current data with previous data\n",
    "    current_paths = set(current_data.keys())\n",
    "    previous_paths = set(previous_data.keys())\n",
    "\n",
    "    # Identify added and removed paths\n",
    "    added_paths = current_paths - previous_paths\n",
    "    removed_paths = previous_paths - current_paths\n",
    "\n",
    "    for path_local in added_paths:\n",
    "        changed_items['added'].append(path_local)\n",
    "    for path_local in removed_paths:\n",
    "        changed_items['removed'].append(path_local)\n",
    "\n",
    "    # Check for modifications in existing paths\n",
    "    for path_local in current_paths.intersection(previous_paths):\n",
    "        current_info = current_data[path_local]\n",
    "        previous_info = previous_data[path_local]\n",
    "        if current_info != previous_info:\n",
    "            changed_items[path_local] = current_info\n",
    "            modified_files = []\n",
    "            for file in current_info['files']:\n",
    "                if file not in previous_info['files'] or \\\n",
    "                    os.path.getmtime(os.path.join(base_path, path_local, file)) != previous_info['last_modified']:\n",
    "                    modified_files.append(file)\n",
    "            if modified_files:\n",
    "                changed_items['modified_files'][path_local] = modified_files\n",
    "    \n",
    "    return changed_items"
   ],
   "id": "d238b9e5fbf63620",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T11:53:28.130112Z",
     "start_time": "2024-04-19T11:53:28.125294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_db(dir_path):\n",
    "    # Check if previous data exists\n",
    "    previous_data = {}\n",
    "    if os.path.exists(os.path.join(dir_path ,'dir_data.pickle')):\n",
    "        with open(os.path.join(dir_path ,'dir_data.pickle'), 'rb') as f:\n",
    "            previous_data = pickle.load(f)\n",
    "    \n",
    "    # Get current directory structure\n",
    "    current_data = {}\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        relative_path = os.path.relpath(root, dir_path)\n",
    "        current_data[relative_path] = {\n",
    "            'dirs': dirs,\n",
    "            'files': files,\n",
    "            'last_modified': os.path.getmtime(root)\n",
    "        }\n",
    "    \n",
    "    # Identify differences\n",
    "    changes = identity_diff(dir_path, previous_data, current_data)\n",
    "    \n",
    "    # Clean MSA folder from changes for the remvoed folder/\n",
    "    changes['removed'] = [x for x in changes['removed'] if 'msas' not in x]\n",
    "    \n",
    "    # Clean MSA folder from changes for the added folder\n",
    "    changes['added'] = [x for x in changes['added'] if 'msas' not in x]\n",
    "    # Save current data for future comparison\n",
    "    with open(os.path.join(dir_path ,'dir_data.pickle'), 'wb') as f:\n",
    "        pickle.dump(current_data, f)\n",
    "    return changes\n"
   ],
   "id": "2e8ad9f438bdd816",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T11:53:29.126057Z",
     "start_time": "2024-04-19T11:53:29.123686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "naming_conventions = \"\"\"Naming conventions for library folders:\n",
    "1. No spaces in the folder names\n",
    "2. Folders that are not predictions are prohibited from using '-' in the folder name\n",
    "\"\"\"\n",
    "print(naming_conventions)"
   ],
   "id": "f4172831fc907e1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naming conventions for library folders:\n",
      "1. No spaces in the folder names\n",
      "2. Folders that are not predictions are prohibited from using '-' in the folder name\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T11:53:29.454236Z",
     "start_time": "2024-04-19T11:53:29.450600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_added_df(changes, library_path):\n",
    "    # Generate a list f paths to reload\n",
    "    reloadable_paths_raw = changes['added'] + list(changes['modified_files'].keys())\n",
    "    reloadable_paths = [os.path.join(library_path, path) for path in reloadable_paths_raw if '-' in path]\n",
    "    \n",
    "    if not reloadable_paths:\n",
    "        return pd.DataFrame()\n",
    "    # Load the library\n",
    "    res = load_library_aslist(reloadable_paths)\n",
    "    unique_df = uniquify(pd.DataFrame(res))\n",
    "    # Remove rows with missing values\n",
    "    unique_df.dropna(subset=['Mean pDockQ', 'pTM', 'ipTM'], inplace=True, ignore_index=True)\n",
    "    # Remove the path prefix\n",
    "    unique_df['Relative Directory'] = unique_df['Directory'].apply(lambda x: x.replace(library_path + os.path.sep, ''))\n",
    "    \n",
    "    return unique_df"
   ],
   "id": "51d78d3fafb0e3c7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Update the Database\n",
    "The library update is a multistep process:\n",
    "1. Identify the changes in the library and store them in a dictionary\n",
    "2. Load the new and modified folders entries into a unique_df dataframe\n",
    "3. Edit the database\n",
    "    1. Load Database as a dataframe\n",
    "    2. Remove the entries that are in the removed directory\n",
    "    3. Add the new entries\n",
    "4. Update Database with modified entries"
   ],
   "id": "ab65c291071c4910"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:49:07.470281Z",
     "start_time": "2024-04-19T12:49:07.464525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_db(library_path):\n",
    "    \n",
    "    # Identify the changes in the database\n",
    "    changes = analyze_db(library_path)\n",
    "    df_toadd = get_added_df(changes, library_path)\n",
    "    \n",
    "    # Load current database\n",
    "    if not os.path.exists(os.path.join(library_path, 'PPIDB_full.db')):\n",
    "        print('Database not found – creating a new one')\n",
    "        cnx = sqlite3.connect(os.path.join(library_path, 'PPIDB_full.db'))\n",
    "        \n",
    "    try:\n",
    "        df_in = pd.read_sql('AF2 full-lib', f\"sqlite:///{os.path.join(library_path, 'PPIDB_full.db')}\")\n",
    "        print(df_in['Job name'].head())\n",
    "        \n",
    "        # Remove the entries that were removed from the database\n",
    "        for rem_path in changes['removed']:\n",
    "            components = os.path.basename(rem_path).split('-')\n",
    "            # Show row where 'Job name' contains both components\n",
    "            index = df_in[df_in['Job name'].str.contains(components[0]) & df_in['Job name'].str.contains(components[1])].index\n",
    "            df_in.drop(index, inplace=True)\n",
    "    except Exception:\n",
    "        print('Database not found – considering it do be empty')\n",
    "        df_in = pd.DataFrame()\n",
    "    \n",
    "    # Add the new entries\n",
    "    df = pd.concat([df_in, df_toadd], ignore_index=True)\n",
    "    print(df['Job name'].head())\n",
    "    \n",
    "    # Save and replace current database with updated one\n",
    "    cnx = sqlite3.connect(os.path.join(library_path, 'PPIDB_full.db'))\n",
    "    df.to_sql(name='AF2 full-lib', con=cnx, if_exists='replace', index=False)"
   ],
   "id": "47737f5673c68f61",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:49:58.442095Z",
     "start_time": "2024-04-19T12:49:58.398321Z"
    }
   },
   "cell_type": "code",
   "source": "update_db('/Volumes/dave/JW/PPIDB/full_lib/control_db')",
   "id": "dcd801b2571f31c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        dnd1-lifeact\n",
      "1      nanos3-lifeact\n",
      "2       lifeact-actb1\n",
      "3    LAscramble-actb1\n",
      "4        dnd1-eif3d_4\n",
      "Name: Job name, dtype: object\n",
      "0        dnd1-lifeact\n",
      "1      nanos3-lifeact\n",
      "2       lifeact-actb1\n",
      "3    LAscramble-actb1\n",
      "Name: Job name, dtype: object\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "27d6d275f4f70cce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

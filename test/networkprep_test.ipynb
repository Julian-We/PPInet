{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:38.313321Z",
     "start_time": "2024-02-21T08:57:38.298339Z"
    }
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "# import requests # Keep requests in case one has to use a ZFIN search in the \"search engine\"\n",
    "import requests\n",
    "import json\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "record = list(SeqIO.parse(\"../fastas/Danio_rerio.GRCz11.pep.all.fa\", \"fasta\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:40.713081Z",
     "start_time": "2024-02-21T08:57:38.763528Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def get_protein_name(protein):\n",
    "    match = re.search('gene_symbol:(.*?)(\\.|\\s|$)' , protein.description)\n",
    "    if match:\n",
    "        return {match.group(1).lower() : protein}\n",
    "    else:\n",
    "        # print(protein.description)\n",
    "        return {protein.id : protein}\n",
    "\n",
    "def all_equal(iterator):\n",
    "    iterator = iter(iterator)\n",
    "    try:\n",
    "        first = next(iterator)\n",
    "    except StopIteration:\n",
    "        return True\n",
    "    return all(first == x for x in iterator)\n",
    "\n",
    "with open('../fastas/ZFIN_1.0.1.4_basicGeneInformation.json') as jsn_fle:\n",
    "    zfin_db = json.load(jsn_fle)\n",
    "    # print(type(zfin_db))\n",
    "\n",
    "def get_uniprot_seq(protein_id):\n",
    "    base_url = \"https://www.uniprot.org/uniprot/\"\n",
    "    query_url = f\"{base_url}{protein_id}.fasta\"\n",
    "\n",
    "    res = requests.get(query_url)\n",
    "    if res.status_code == 200:\n",
    "        return ''.join(res.text.splitlines()[1:])\n",
    "    else:\n",
    "        print('Protein sequence not found!')\n",
    "\n",
    "\n",
    "def get_unique_items(input_list):\n",
    "    unique_items = []\n",
    "    seen_items = set()\n",
    "\n",
    "    for item in input_list:\n",
    "        item_seq = item.seq\n",
    "        if item_seq not in seen_items:\n",
    "            unique_items.append(item)\n",
    "            seen_items.add(item_seq)\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "def get_update_sequence(q_gene, s_gene):\n",
    "    \"\"\"\n",
    "\n",
    "    :param q_gene:  query gene\n",
    "    :param s_gene: searched gene\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    multi= []\n",
    "    if list(q_gene.keys())[0] == s_gene:\n",
    "        multi.append(q_gene.get(list(q_gene.keys())[0]).seq)\n",
    "    for var_num, protein_seq in enumerate(get_unique_items(multi)):\n",
    "        if var_num == 0 and protein_seq is not None and len(protein_seq) > 1:\n",
    "            return {s_gene: SeqIO.SeqRecord(Seq(protein_seq), id=s_gene, description='')}\n",
    "        elif var_num != 0 and protein_seq is not None and len(protein_seq) > 1:\n",
    "            return {f'{s_gene}_{var_num}': SeqIO.SeqRecord(Seq(protein_seq), id=s_gene, description='')}\n",
    "\n",
    "\n",
    "def dict_get(di, key):\n",
    "    if di.get(key) is not None:\n",
    "        return di.get(key)#.seq\n",
    "\n",
    "def phoenix(record_dict, gene_list):\n",
    "    gnnme = list(record_dict.keys())[0]\n",
    "\n",
    "    all_multiplcates = list(map(dict_get, gene_list, len(gene_list)*[gnnme]))\n",
    "    all_multiplcates = [i for i in all_multiplcates if i is not None]\n",
    "\n",
    "    unique_multis = get_unique_items(all_multiplcates)\n",
    "\n",
    "    unique_multis = [q for q in unique_multis if q is not None]\n",
    "\n",
    "    out_dict = {}\n",
    "    for var_num, mult in enumerate(unique_multis):\n",
    "        if var_num == 0 and mult is not None and len(mult) > 1:\n",
    "            # out_dict.update({gnnme: SeqIO.SeqRecord(Seq(mult), id=gnnme, description='')})\n",
    "            out_dict.update({gnnme: mult})\n",
    "        elif var_num != 0 and mult is not None and len(mult) > 1:\n",
    "            # out_dict.update({f'{gnnme}_{var_num}': SeqIO.SeqRecord(Seq(mult), id=gnnme, description='')})\n",
    "            out_dict.update({f'{gnnme}_{var_num}': mult})\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:43.241991Z",
     "start_time": "2024-02-21T08:57:40.728797Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DICT_GENES = {} # This stays no matter what\n",
    "list_genes = list(map(get_protein_name, record))\n",
    "\n",
    "# # Version 1 of how to generate the DICT_GENES via data from a proteome fasta\n",
    "# for d in tqdm(list_genes):\n",
    "#     registered = False\n",
    "#     turn = 1\n",
    "#     while not registered:\n",
    "#         local_gene_name = list((d.keys()))[0]\n",
    "#         if DICT_GENES.get(local_gene_name) is None:\n",
    "#             DICT_GENES.update(d)\n",
    "#             registered = True\n",
    "#         else:\n",
    "#             if DICT_GENES.get(f'{local_gene_name}_{turn}') is None:\n",
    "#                 DICT_GENES.update({f'{local_gene_name}_{turn}': d.get(local_gene_name)})\n",
    "#                 registered = True\n",
    "#             else:\n",
    "#                 turn +=1\n",
    "\n",
    "# # Version 2 of how to create the DICT_GENES via ZFIN DB and UniProt Pulls\n",
    "# for zfin_entry in tqdm(zfin_db['data']):\n",
    "#     gene_name = zfin_entry['symbol']\n",
    "#     # print(zfin_entry['basicGeneticEntity']['crossReferences'])\n",
    "#     multiple_entries = []\n",
    "#     for uniprot_entry in zfin_entry['basicGeneticEntity']['crossReferences']:\n",
    "#         if 'uniprot' in uniprot_entry['id'].lower():\n",
    "#             matcha = re.match( '(UniProt.{2}):(.{6})', uniprot_entry['id'])\n",
    "#             sequence = get_uniprot_seq(matcha.group(2))\n",
    "#             multiple_entries.append(sequence)\n",
    "#     for var_num, protein_seq in enumerate(get_unique_items(multiple_entries)):\n",
    "#         if var_num == 0 and protein_seq is not None and len(protein_seq) > 1:\n",
    "#             DICT_GENES.update({gene_name: SeqIO.SeqRecord(Seq(protein_seq), id=gene_name, description='')})\n",
    "#         elif var_num != 0 and protein_seq is not None and len(protein_seq) > 1:\n",
    "#             DICT_GENES.update({f'{gene_name}_{var_num}': SeqIO.SeqRecord(Seq(protein_seq), id=gene_name, description='')})\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:44.133193Z",
     "start_time": "2024-02-21T08:57:43.231811Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Version 1.5 via proteome fasta- filtering extra sequences\n",
    "# len_list_genes = len(list_genes)\n",
    "# pre_dicts = list(map(phoenix, tqdm(list_genes), len_list_genes*[list_genes]))\n",
    "#\n",
    "# for pre_dict in pre_dicts:\n",
    "#     DICT_GENES.update(pre_dict)\n",
    "#\n",
    "# print(len(list_genes))\n",
    "# print(len(pre_dicts))\n",
    "# print(len(DICT_GENES))\n",
    "#\n",
    "#\n",
    "# # Dump into json\n",
    "#\n",
    "# # Writing to sample.json\n",
    "# with open(\"DICT_GENES.pkl\", \"wb\") as out_jsn_fle:\n",
    "#     pickle.dump(DICT_GENES, out_jsn_fle)\n",
    "\n",
    "\n",
    "# Verison 3 of how to create the DICT_GENES via recovery from previous endevers\n",
    "with open('../DICT_GENES.pkl', 'rb') as in_jsn_fle:\n",
    "    # Reading from json file\n",
    "    DICT_GENES = pickle.load(in_jsn_fle)\n",
    "    if type(DICT_GENES) == type(dict):\n",
    "        raise TypeError(f'DICT_GENES does not have the right type. Instead it has type {type(DICT_GENES)}')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:46.738794Z",
     "start_time": "2024-02-21T08:57:43.836564Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# This section generates a list of canonical transcripts\n",
    "#\n",
    "# def check_canonical_label(transcript_ids_set):\n",
    "#     transcript_ids = list(transcript_ids_set)\n",
    "#     # Ensembl REST API endpoint for fetching transcript information\n",
    "#     endpoint = f\"https://rest.ensembl.org/lookup/id/\"\n",
    "#     headers={ \"Content-Type\" : \"application/json\", \"Accept\" : \"application/json\"}\n",
    "#     r = requests.post(endpoint, headers=headers, data='{\"ids\" : ' + '[' + ', '.join(f'\"{elem}\"' for elem in transcript_ids) + ']' + '}') #str({ \"ids\" : transcript_ids })\n",
    "#     # Make a GET request to the endpoint\n",
    "#     # response = requests.get(endpoint, headers={ \"Content-Type\" : \"application/json\"})\n",
    "#     # Check if the request was successful (status code 200)\n",
    "#     if r.status_code == 200:\n",
    "#         out_dict = {}\n",
    "#         print(r.status_code)\n",
    "#         transcript_info = r.json()\n",
    "#         for key, value in transcript_info.items():\n",
    "#             out_dict.update({key: value.get('canonical_transcript')})\n",
    "#         # print(transcript_info)#.get('canonical_transcript'))\n",
    "#         return out_dict\n",
    "#     else:\n",
    "#         print(r.status_code)\n",
    "#         return None\n",
    "#\n",
    "# ensemble_names = []\n",
    "# for entry in list(DICT_GENES.values()):\n",
    "#     desc = entry.description\n",
    "#     desc_cut_1 = desc[desc.find('gene:')+5:]\n",
    "#     desc_cut_2 = desc_cut_1[:desc_cut_1.find('.')]\n",
    "#     ensemble_names.append(desc_cut_2)\n",
    "#\n",
    "# # Create subsets\n",
    "# list_of_ensmbl_subsets = []\n",
    "# counter = 0\n",
    "# subset = set()\n",
    "# check_sum = 0\n",
    "# neg_sum = 0\n",
    "# for gene_name in ensemble_names:\n",
    "#     if counter < 50:\n",
    "#         subset.add(gene_name)\n",
    "#         counter += 1\n",
    "#     elif len(gene_name) != 18:\n",
    "#         neg_sum +=1\n",
    "#     else:\n",
    "#         list_of_ensmbl_subsets.append(subset)\n",
    "#         check_sum += len(subset)\n",
    "#         # print(len(subset))\n",
    "#         subset = {gene_name}\n",
    "#         counter = 1\n",
    "# if len(subset) != 0:\n",
    "#     list_of_ensmbl_subsets.append(subset)\n",
    "#\n",
    "# list_canonical_dicts = list(map(check_canonical_label, tqdm(list_of_ensmbl_subsets)))\n",
    "#\n",
    "# CANONICAL_DICT = {}\n",
    "# for canonical_gene in list_canonical_dicts:\n",
    "#    CANONICAL_DICT.update(canonical_gene)\n",
    "#\n",
    "# with open(\"canonical_transcripts.pkl\", \"wb\") as out_jsn_fle_canon:\n",
    "#     pickle.dump(CANONICAL_DICT, out_jsn_fle_canon)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:46.740930Z",
     "start_time": "2024-02-21T08:57:46.734522Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "with open(\"../canonical_transcripts.pkl\", \"rb\") as in_jsn_fle_canon:\n",
    "    CANONICAL_DICT = pickle.load(in_jsn_fle_canon)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:46.770135Z",
     "start_time": "2024-02-21T08:57:46.734798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# https://clinicaltables.nlm.nih.gov/api/ncbi_genes/v3/search?\n",
    "def search_engine(search_query):\n",
    "    foundit = False\n",
    "    search_query = search_query.lower()\n",
    "    while not foundit:\n",
    "        if DICT_GENES.get(search_query) is not None:\n",
    "            # print(f'found {search_query}')\n",
    "            regex_search = f'{search_query}'+'_\\d{1,3}'\n",
    "            hit_list = [search_query]\n",
    "            for some_gene_name in DICT_GENES.keys():\n",
    "                if re.match(regex_search, some_gene_name) is not None:\n",
    "                    hit_list.append(some_gene_name)\n",
    "            foundit = True\n",
    "            return hit_list\n",
    "\n",
    "\n",
    "\n",
    "        response_selection = []\n",
    "        for gene_name, gene_seqio in DICT_GENES.items():\n",
    "            if search_query in gene_seqio.description.lower():\n",
    "                if re.search('(.*)(_\\d*)' , gene_name) is None:\n",
    "                    response_selection.append(gene_name)\n",
    "            elif search_query.replace(\" \",\"\") in gene_seqio.description.lower().replace(\" \",\"\"):\n",
    "                response_selection.append(gene_name)\n",
    "            elif search_query in gene_name.lower():\n",
    "                response_selection.append(gene_name)\n",
    "        variants_dict = {}\n",
    "        # print(response_selection)\n",
    "        for variant in response_selection:\n",
    "            does_match = re.match(r'(.*?)(_\\d*)', variant)\n",
    "            if does_match:\n",
    "                prefix_name = does_match.group(1)\n",
    "                variants_dict.setdefault(prefix_name, []).append(variant)\n",
    "            else:\n",
    "                variants_dict.update({variant: [variant]})\n",
    "        if list(variants_dict.keys()):\n",
    "            precise_name = input(f'Search query: {search_query}\\n If your gene is in this list type its exact name:{list(variants_dict.keys())} \\n If not, press \"ok\".')\n",
    "        else:\n",
    "            print(f'No result for {search_query} was found, thus its not considered')\n",
    "            return [None]\n",
    "\n",
    "        if precise_name == '':\n",
    "            search_query = input('Please enter an alternative search query or give up by pressing \"Ok\"')\n",
    "            if search_query == '':\n",
    "                foundit = True\n",
    "        else:\n",
    "            if DICT_GENES.get(precise_name) is not None:\n",
    "                return variants_dict.get(precise_name)\n",
    "            else:\n",
    "                search_query = input(f'The name was not found in the databank. Perhaps a typo in {precise_name}?')\n",
    "\n",
    "\n",
    "\n",
    "def searchengine_master(candidate_list, canonical):\n",
    "    clean_candidate_list = []\n",
    "    for candidate in candidate_list:\n",
    "        se_result = search_engine(candidate)\n",
    "        for ses_result in se_result:\n",
    "            if ses_result is not None:\n",
    "                se_desc = DICT_GENES.get(ses_result).description\n",
    "                desc_cut_1 = se_desc[se_desc.find('gene:')+5:]\n",
    "                gene_name = desc_cut_1[:desc_cut_1.find('.')]\n",
    "                se_tr_cut1 = se_desc[se_desc.find('transcript:')+11:]\n",
    "                transcript_name = se_tr_cut1[:se_tr_cut1.find(' ')]\n",
    "                if ses_result is not None and canonical and CANONICAL_DICT.get(gene_name) == transcript_name:\n",
    "                    clean_candidate_list.append(ses_result)\n",
    "\n",
    "    return clean_candidate_list\n",
    "\n",
    "def af_computetime(len_seq):\n",
    "    # print(len_seq)\n",
    "    range_to_m = {\n",
    "        (0, 100): 4.9,\n",
    "        (100, 200): 7.7,\n",
    "        (200, 300): 13,\n",
    "        (300, 400): 18,\n",
    "        (400, 500): 29,\n",
    "        (500, 600): 39,\n",
    "        (600, 700): 53,\n",
    "        (700, 800): 60,\n",
    "        (800, 900): 91,\n",
    "        (900, 1000): 96,\n",
    "        (1000, 1100): 140,\n",
    "        (1100, 1500): 280,\n",
    "        (1500, 2000): 450,\n",
    "        (2000, 2500): 969,\n",
    "        (2500, 3000): 1240,\n",
    "        (3000, 3500): 2465,\n",
    "        (3500, 4000): 5660,\n",
    "        (4000, 4500): 12475,\n",
    "        (4500, 5000): 18824,\n",
    "        (5000, 12000): 50000\n",
    "    }\n",
    "\n",
    "    for (low, high), value in range_to_m.items():\n",
    "        if low < len_seq < high:\n",
    "            len_prediction = range_to_m.get((low, high)) * 5 * 3\n",
    "            # Taking 0.2min per residue for the MSA as an estimation\n",
    "            msa_prediction = 15 * len_seq\n",
    "            # predicted time +10%  tolerance\n",
    "            return (len_prediction + msa_prediction) * 1.1\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def target_system(target_gene, missile_gene, separation='entry'):\n",
    "    try:\n",
    "        missile_sequence = str(DICT_GENES.get(missile_gene).seq)\n",
    "        target_sequence = str(DICT_GENES.get(target_gene).seq)\n",
    "        if separation == ':':\n",
    "            return {''.join([missile_gene, '-', target_gene]): ''.join([missile_sequence, ':', target_sequence])}\n",
    "        else:\n",
    "            missile_object = SeqIO.SeqRecord(Seq(missile_sequence), id=missile_gene, description='')\n",
    "            target_object = SeqIO.SeqRecord(Seq(target_sequence), id=target_gene, description='')\n",
    "            return {''.join([missile_gene, '-', target_gene]): [missile_object, target_object]}\n",
    "    except AttributeError as e:\n",
    "        print(f'Problem with mseq: {missile_gene}; tseq: {target_gene}')\n",
    "\n",
    "\n",
    "def seconds_to_hms(secs):\n",
    "    hours, remainder = divmod(secs, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return \"{:02d}:{:02d}:{:02d}\".format(int(hours), int(minutes), int(seconds))\n",
    "\n",
    "\n",
    "def af_statistics(seq_dict):\n",
    "    \"\"\"\n",
    "    :param seq_dic:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    str_total_len = f'The are a total of {len(seq_dict)} fasta files being folded'\n",
    "    # print(str_total_len)\n",
    "    total_legnth = 0\n",
    "    total_runtime = 0\n",
    "    for sequence in seq_dict.values():\n",
    "        if type(sequence) != type(list()):\n",
    "            tmp_len = len(sequence) - 1\n",
    "        else:\n",
    "            tmp_len = 0\n",
    "            for sequ in sequence:\n",
    "                tmp_len += len(sequ)\n",
    "        total_legnth += tmp_len\n",
    "        total_runtime += af_computetime(tmp_len)\n",
    "        # print(seconds_to_hms(round(af_computetime(tmp_len), -2)))\n",
    "    total_runtime += total_runtime * 0.12 # Plus 12% buffer in time since a few jobs timed out\n",
    "    str_avg_len = f'With an average lenth of {total_legnth/len(seq_dict)}'\n",
    "    str_time = f'And a predicted total runtime of {seconds_to_hms(total_runtime)}'\n",
    "    l_master_str = [str_total_len, '\\n', str_avg_len, '\\n', str_time]\n",
    "    # return ''.join(l_master_str)\n",
    "    return [seconds_to_hms(total_runtime), total_runtime]\n",
    "def create_bash_script(experiment_name, comb_name, run_time, path, partition = \"gpua100\", user = \"jwegner1@uni-muenster.de\", nodes = 1, cores = 10, gres = 1, memory = 60, precomputed_msas = False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param experiment_name:\n",
    "    :param comb_name:\n",
    "    :param run_time:\n",
    "    :param path:\n",
    "    :param partition:\n",
    "    :param user:\n",
    "    :param nodes:\n",
    "    :param cores:\n",
    "    :param gres:\n",
    "    :param memory:\n",
    "    :param precomputed_msas:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    job_name = f\"job_{comb_name}\"\n",
    "    pre_msas = str(precomputed_msas).lower()\n",
    "\n",
    "    # bash_script_lines = [\"#!/bin/bash \\n\",\n",
    "    #                      f\"#SBATCH --partition={partition} \\n\",\n",
    "    #                      f\"#SBATCH --nodes={nodes} \\n\",\n",
    "    #                      f\"#SBATCH --gres=gpu:{gres} \\n\",\n",
    "    #                      f\"#SBATCH --cpus-per-task={cores} \\n\",\n",
    "    #                      f\"#SBATCH --mem={memory}G \\n\",\n",
    "    #                      f\"#SBATCH --time={run_time} \\n\",\n",
    "    #                      f\"#SBATCH --job-name={job_name} \\n\",\n",
    "    #                      \"#SBATCH --account=uni \\n\",\n",
    "    #                      \"#SBATCH --mail-type=ALL \\n\",\n",
    "    #                      f\"#SBATCH --mail-user={user} \\n\",\n",
    "    #                      \" \\n\",\n",
    "    #                      \"module load palma/2021a \\n\",\n",
    "    #                      \"module load foss/2021a \\n\",\n",
    "    #                      \"module load AlphaFold/2.1.2 \\n\",\n",
    "    #                      \"wait \\n\",\n",
    "    #                      \"export ALPHAFOLD_DATA_DIR=/Applic.HPC/data/alphafold \\n\",\n",
    "    #                      \"\\n\",\n",
    "    #                      \"alphafold \\\\n\",\n",
    "    #                      f\"    --fasta_paths=/scratch/tmp/jwegner1/{experiment_name}/fasta/{comb_name}.fasta \\\\n\",\n",
    "    #                      \"    --model_preset=multimer \\\\n\",\n",
    "    #                      f\"    --output_dir=/scratch/tmp/jwegner1/{experiment_name}/xprt \\\\n\",\n",
    "    #                      f\"    --use_precomputed_msas={pre_msas} \\\\n\",\n",
    "    #                      \"    --max_template_date=2021-11-25 \\\\n\",\n",
    "    #                      \"    --is_prokaryote_list=false \\\\n\",\n",
    "    #                      \"    --db_preset=reduced_dbs \\\\n\",\n",
    "    #                      \"    --data_dir=/Applic.HPC/data/alphafold\\n\"\n",
    "    #                      ]\n",
    "    bash_script_text = f\"\"\"#!/bin/bash\n",
    "#SBATCH --partition={partition}\n",
    "#SBATCH --nodes={nodes}\n",
    "#SBATCH --gres=gpu:{gres}\n",
    "#SBATCH --cpus-per-task={cores}\n",
    "#SBATCH --mem={memory}G\n",
    "#SBATCH --time={run_time}\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --account=uni\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user={user}\n",
    "\n",
    "module load palma/2021a\n",
    "module load foss/2021a\n",
    "module load AlphaFold/2.1.2\n",
    "wait\n",
    "export ALPHAFOLD_DATA_DIR=/Applic.HPC/data/alphafold\n",
    "\n",
    "alphafold \\\\\n",
    "    --fasta_paths=/scratch/tmp/jwegner1/{experiment_name}/fasta/{comb_name}.fasta \\\\\n",
    "    --model_preset=multimer \\\\\n",
    "    --output_dir=/scratch/tmp/jwegner1/{experiment_name}/xprt \\\\\n",
    "    --max_template_date=2021-11-25 \\\\\n",
    "    --use_precomputed_msas={pre_msas} \\\\\n",
    "    --is_prokaryote_list=false \\\\\n",
    "    --db_preset=reduced_dbs \\\\\n",
    "    --data_dir=/Applic.HPC/data/alphafold\n",
    "\"\"\"\n",
    "\n",
    "    with open(os.path.join(path,f'{job_name}.sh'), 'w+') as bsh:\n",
    "        # bsh.writelines(bash_script_lines)\n",
    "        bsh.write(bash_script_text)\n",
    "    return os.path.join('/scratch/tmp/jwegner1/af2_control_net/bash_scripts/', f'{job_name}.sh')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:46.846595Z",
     "start_time": "2024-02-21T08:57:46.807110Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def export_fastas(pre_fasta_dict, output_directory):\n",
    "    try:\n",
    "        fasta_path = os.path.join(output_directory, 'fasta')\n",
    "        os.makedirs(fasta_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    for fa_name, fa_sequence in pre_fasta_dict.items():\n",
    "        fa_path = os.path.join(fasta_path, f'{fa_name}.fasta')\n",
    "        if type(fa_sequence) == type(str()):\n",
    "            fa_record = SeqIO.SeqRecord(Seq(fa_sequence), id=fa_name, description='')\n",
    "            SeqIO.write(fa_record, fa_path, 'fasta-2line')\n",
    "        elif type(fa_sequence) == type(list()):\n",
    "            # print(fa_sequence)\n",
    "            SeqIO.write(fa_sequence, fa_path, 'fasta-2line')\n",
    "    # msg = af_statistics(pre_fasta_dict)\n",
    "    try:\n",
    "        bash_path = os.path.join(output_directory, 'bash_scripts')\n",
    "        os.makedirs(bash_path)\n",
    "        os.makedirs(os.path.join(output_directory, 'xprt'))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    master_bash_lines = ['#!/bin/bash \\n']\n",
    "    # Quick and dirty time to assesment\n",
    "    tot_runtime = 0\n",
    "    for k, v in pre_fasta_dict.items():\n",
    "        msg = af_statistics({k:v})\n",
    "        if msg[1] < 5500:\n",
    "            print(f'----- {k} underbet t_min with : \\t {msg[0]}')\n",
    "            msg = [seconds_to_hms(5500), 5500]\n",
    "        print(f'{k} needs: \\t {msg[0]}')\n",
    "        sh_pth = create_bash_script(os.path.basename(output_directory),k,msg[0],bash_path)\n",
    "        master_bash_lines.append('sbatch ' + sh_pth + ' \\n')\n",
    "        # print(msg)\n",
    "        tot_runtime += msg[1]\n",
    "\n",
    "    with open(os.path.join(output_directory,f'dealer.sh'), 'w+') as bsh:\n",
    "        bsh.writelines(master_bash_lines)\n",
    "\n",
    "    for x in range(3):\n",
    "        print('')\n",
    "    print(f'The job will take approximately {seconds_to_hms(tot_runtime)}')\n",
    "    print(f'Folding {len(pre_fasta_dict)} pairs')\n",
    "\n",
    "\n",
    "\n",
    "    # with open(os.path.join(output_directory,'computing_prediction.txt'), \"w\") as cp_file:\n",
    "    #     print(msg)\n",
    "    #     cp_file.write(msg)\n",
    "    #     cp_file.close()\n",
    "\n",
    "def some_vs_some(output_directory: str, genes_group1: list, genes_group2: list, only_canonical: bool, seq_mode='entry'):\n",
    "    \"\"\"\n",
    "    Generate fasta files to feed into alphafold 2.\n",
    "    :param only_canonical:\n",
    "    :param output_directory:\n",
    "    :param genes_group1:\n",
    "    :param genes_group2:\n",
    "    :param seq_mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    clean_gene_group1 = searchengine_master(genes_group1, only_canonical)\n",
    "    clean_gene_group2 = searchengine_master(genes_group2, only_canonical)\n",
    "\n",
    "    # For each gene in this list generate all interacting\n",
    "    pre_fasta_dict = {}\n",
    "    for gene1 in clean_gene_group1:\n",
    "        # print('tick')\n",
    "        combination_list = list(map(target_system, clean_gene_group2, len(clean_gene_group2) * [gene1]))\n",
    "        for combi_dict in combination_list:\n",
    "            pre_fasta_dict.update(combi_dict)\n",
    "\n",
    "    export_fastas(pre_fasta_dict, output_directory)\n",
    "\n",
    "\n",
    "def all_vs_all(output_directory: str, gene_group: list, only_canonical: bool):\n",
    "    clean_gene_group = searchengine_master(gene_group, only_canonical)\n",
    "    print(clean_gene_group)\n",
    "\n",
    "    # For each gene in this list generate all interacting\n",
    "    pre_fasta_dict = {}\n",
    "    for gene1 in clean_gene_group:\n",
    "        # print('tick')\n",
    "        combination_list = list(map(target_system, clean_gene_group, len(clean_gene_group) * [gene1]))\n",
    "        for combi_dict in combination_list:\n",
    "            pre_fasta_dict.update(combi_dict)\n",
    "\n",
    "    export_fastas(pre_fasta_dict, output_directory)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:54.448033Z",
     "start_time": "2024-02-21T08:57:54.292620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "zf_npc_components = ['aaas', 'gle1', 'ndc1', 'nup35', 'nup37', 'nup42', 'nup43', 'nup50', 'nup54', 'nup58', 'nup62l', 'nup85', 'nup88', 'nup93', 'nup98', 'nup107', 'nup133', 'nup153', 'nup160', 'nup188', 'nup205', 'nup210', 'nup214', 'nup358', 'pom121', 'rae1', 'sec13', 'seh1l', 'tpra', 'tprb']\n",
    "\n",
    "zf_germ_cell_components = ['dnd1', 'nanos3', 'piwil1', 'buc', 'gra', 'ddx4', 'tdrd7a', 'tdrd6', 'tia1', 'dazl', 'dazap1', 'dynll2a', 'hook2', 'ranbp9', ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:55.708116Z",
     "start_time": "2024-02-21T08:57:55.678960Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No result for h2bc1 was found, thus its not considered\n",
      "['dnd1', 'nanos3', 'cxcr4b', 'cxcl12a', 'ddx4_5', 'actb1_3', 'pou5f3']\n",
      "dnd1-dnd1 needs: \t 04:41:12\n",
      "dnd1-nanos3 needs: \t 03:07:34\n",
      "dnd1-cxcr4b needs: \t 04:13:47\n",
      "dnd1-cxcl12a needs: \t 02:49:05\n",
      "dnd1-ddx4_5 needs: \t 07:13:21\n",
      "dnd1-actb1_3 needs: \t 04:20:34\n",
      "dnd1-pou5f3 needs: \t 04:59:59\n",
      "nanos3-dnd1 needs: \t 03:07:34\n",
      "nanos3-nanos3 needs: \t 01:43:29\n",
      "nanos3-cxcr4b needs: \t 02:49:42\n",
      "----- nanos3-cxcl12a underbet t_min with : \t 01:23:28\n",
      "nanos3-cxcl12a needs: \t 01:31:40\n",
      "nanos3-ddx4_5 needs: \t 04:57:31\n",
      "nanos3-actb1_3 needs: \t 02:56:29\n",
      "nanos3-pou5f3 needs: \t 03:30:40\n",
      "cxcr4b-dnd1 needs: \t 04:13:47\n",
      "cxcr4b-nanos3 needs: \t 02:49:42\n",
      "cxcr4b-cxcr4b needs: \t 03:55:55\n",
      "cxcr4b-cxcl12a needs: \t 02:28:08\n",
      "cxcr4b-ddx4_5 needs: \t 06:12:22\n",
      "cxcr4b-actb1_3 needs: \t 04:02:42\n",
      "cxcr4b-pou5f3 needs: \t 04:42:07\n",
      "cxcl12a-dnd1 needs: \t 02:49:05\n",
      "----- cxcl12a-nanos3 underbet t_min with : \t 01:23:28\n",
      "cxcl12a-nanos3 needs: \t 01:31:40\n",
      "cxcl12a-cxcr4b needs: \t 02:28:08\n",
      "----- cxcl12a-cxcl12a underbet t_min with : \t 01:03:21\n",
      "cxcl12a-cxcl12a needs: \t 01:31:40\n",
      "cxcl12a-ddx4_5 needs: \t 04:39:02\n",
      "cxcl12a-actb1_3 needs: \t 02:34:55\n",
      "cxcl12a-pou5f3 needs: \t 03:07:52\n",
      "ddx4_5-dnd1 needs: \t 07:13:21\n",
      "ddx4_5-nanos3 needs: \t 04:57:31\n",
      "ddx4_5-cxcr4b needs: \t 06:12:22\n",
      "ddx4_5-cxcl12a needs: \t 04:39:02\n",
      "ddx4_5-ddx4_5 needs: \t 08:47:17\n",
      "ddx4_5-actb1_3 needs: \t 06:19:08\n",
      "ddx4_5-pou5f3 needs: \t 07:32:08\n",
      "actb1_3-dnd1 needs: \t 04:20:34\n",
      "actb1_3-nanos3 needs: \t 02:56:29\n",
      "actb1_3-cxcr4b needs: \t 04:02:42\n",
      "actb1_3-cxcl12a needs: \t 02:34:55\n",
      "actb1_3-ddx4_5 needs: \t 06:19:08\n",
      "actb1_3-actb1_3 needs: \t 04:09:28\n",
      "actb1_3-pou5f3 needs: \t 04:48:54\n",
      "pou5f3-dnd1 needs: \t 04:59:59\n",
      "pou5f3-nanos3 needs: \t 03:30:40\n",
      "pou5f3-cxcr4b needs: \t 04:42:07\n",
      "pou5f3-cxcl12a needs: \t 03:07:52\n",
      "pou5f3-ddx4_5 needs: \t 07:32:08\n",
      "pou5f3-actb1_3 needs: \t 04:48:54\n",
      "pou5f3-pou5f3 needs: \t 05:20:19\n",
      "\n",
      "\n",
      "\n",
      "The job will take approximately 208:05:02\n",
      "Folding 49 pairs\n"
     ]
    }
   ],
   "source": [
    "all_vs_all('/Users/izbuser/Desktop/af2_control_net', ['dnd1', 'nanos3', 'cxcr4b', 'cxcl12a', 'ddx4', 'actb1', 'pou5f3', 'h2bc1'], True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T08:57:58.221976Z",
     "start_time": "2024-02-21T08:57:56.711176Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_vs_all('npc_interactome',zf_npc_components, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T07:59:06.741255Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'some_vs_some' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/62/yv6r5_y115s_tyrp10l5d9z40000gp/T/ipykernel_10520/3166763593.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msome_vs_some\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/Volumes/MartyMcDrve/experiments/tdrd7a_interactome_small'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'tdrd7a'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mzf_npc_components\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'some_vs_some' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "some_vs_some('/Volumes/MartyMcDrve/experiments/tdrd7a_interactome_small', ['tdrd7a'], zf_npc_components, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T13:22:06.498467Z",
     "start_time": "2024-02-23T13:22:06.306386Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for k, v in DICT_GENES.items():\n",
    "#     if 'seh1' in k.lower():\n",
    "#         print(k, v)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T07:59:06.761809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "some_vs_some('granules_vs_npc_interactome', zf_germ_cell_components, zf_npc_components, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T07:59:06.765953Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_vs_all('granule_interactome', zf_germ_cell_components, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T07:59:06.768490Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "\"['dnd1', 'nanos3', 'piwil1', 'buc', 'gra', 'ddx4', 'tdrd7a', 'tdrd6', 'tia1', 'dazl', 'dazap1', 'dynll2a', 'hook2', 'ranbp9']\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"dnd1\", 'nanos3', 'piwil1', 'buc', 'gra', 'ddx4', 'tdrd7a', 'tdrd6', 'tia1', 'dazl', 'dazap1', 'dynll2a', 'hook2', 'ranbp9', ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:52:21.461660Z",
     "start_time": "2024-02-22T15:52:21.405797Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Build this into the search engine to create mutations\n",
    "search_stuff = \"dnd1_3|Y72D|X123Y|\"\n",
    "matches = re.findall(\"\\|\\w\\d*\\w\\|\", search_stuff)\n",
    "\n",
    "for mut in matches:\n",
    "    before_mut = mut[1]\n",
    "    after_mut = mut[-2]\n",
    "    num_mut = int(mut[2:-2])\n",
    "    \n",
    "    print(before_mut, after_mut, num_mut)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T16:26:03.131287Z",
     "start_time": "2024-04-22T16:26:03.129519Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
